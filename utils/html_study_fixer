import requests
from bs4 import BeautifulSoup
import fitz  # PyMuPDF
import re
import time
import os
import argparse
from urllib.parse import urlparse
from datetime import datetime

def get_pubmed_info(pmid):
    try:
        url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id={pmid}&retmode=json"
        response = requests.get(url)
        data = response.json()
        result = data['result'][pmid]
        journal = result.get('source', 'Unknown Journal')
        pubdate = result.get('pubdate', 'Unknown Date')
        return journal, pubdate  # Return full date
    except:
        return None, None

def get_semanticscholar_info(paper_id):
    try:
        url = f"https://api.semanticscholar.org/v1/paper/{paper_id}"
        response = requests.get(url)
        data = response.json()
        journal = data.get('venue', 'Unknown Journal')
        year = data.get('year', 'Unknown Date')
        return journal, str(year)
    except:
        return None, None

def get_crossref_info(doi):
    try:
        url = f"https://api.crossref.org/works/{doi}"
        response = requests.get(url)
        data = response.json()
        journal = data['message'].get('container-title', ['Unknown Journal'])[0]
        date_parts = data['message'].get('issued', {}).get('date-parts', [[None]])[0]
        
        # Try to get full date (year, month, day)
        if len(date_parts) >= 3:
            year, month, day = date_parts[0], date_parts[1], date_parts[2]
            pub_date = f"{year}-{month:02d}-{day:02d}"
        elif len(date_parts) >= 2:
            year, month = date_parts[0], date_parts[1]
            pub_date = f"{year}-{month:02d}"
        elif len(date_parts) >= 1 and date_parts[0]:
            pub_date = str(date_parts[0])
        else:
            pub_date = 'Unknown Date'
            
        return journal, pub_date
    except:
        return None, None

def extract_doi_from_webpage(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        doi_meta = soup.find('meta', {'name': 'citation_doi'})
        if doi_meta:
            return doi_meta['content']
        return None
    except:
        return None

def parse_pdf(pdf_url):
    try:
        response = requests.get(pdf_url)
        doc = fitz.open(stream=response.content, filetype="pdf")
        text = ""
        for page in doc:
            text += page.get_text()
        
        # Try to find journal
        journal = 'Unknown Journal'
        patterns = [
            r'Journal:\s*(.+)',
            r'Published in:\s*(.+)\n',
            r'@article\{.*?journal\s*=\s*{([^}]+)}',
        ]
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                journal = match.group(1).strip()
                break
        
        # Try to find full date (YYYY-MM-DD)
        pub_date = 'Unknown Date'
        # Look for ISO format date
        date_match = re.search(r'(20\d{2}|19\d{2})[-/](0[1-9]|1[0-2])[-/](0[1-9]|[12][0-9]|3[01])', text)
        if date_match:
            year, month, day = date_match.group(1), date_match.group(2), date_match.group(3)
            pub_date = f"{year}-{month}-{day}"
        else:
            # Look for month name, day, year format
            month_names = r'(January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Mar|Apr|Jun|Jul|Aug|Sep|Oct|Nov|Dec)'
            # Use raw string for regex pattern
            date_match = re.search(r"{0}\s+(\d{{1,2}}),?\s+(20\d{{2}}|19\d{{2}})".format(month_names), text, re.IGNORECASE)
            if date_match:
                month_name, day, year = date_match.group(1), date_match.group(2), date_match.group(3)
                month_dict = {
                    'january': '01', 'jan': '01', 'february': '02', 'feb': '02', 'march': '03', 'mar': '03',
                    'april': '04', 'apr': '04', 'may': '05', 'june': '06', 'jun': '06', 'july': '07', 'jul': '07',
                    'august': '08', 'aug': '08', 'september': '09', 'sep': '09', 'october': '10', 'oct': '10',
                    'november': '11', 'nov': '11', 'december': '12', 'dec': '12'
                }
                month = month_dict.get(month_name.lower(), '01')
                pub_date = f"{year}-{month}-{int(day):02d}"
            else:
                # If no full date, try to find just year and month
                date_match = re.search(r"{0}\s+(20\d{{2}}|19\d{{2}})".format(month_names), text, re.IGNORECASE)
                if date_match:
                    month_name, year = date_match.group(1), date_match.group(2)
                    month_dict = {
                        'january': '01', 'jan': '01', 'february': '02', 'feb': '02', 'march': '03', 'mar': '03',
                        'april': '04', 'apr': '04', 'may': '05', 'june': '06', 'jun': '06', 'july': '07', 'jul': '07',
                        'august': '08', 'aug': '08', 'september': '09', 'sep': '09', 'october': '10', 'oct': '10',
                        'november': '11', 'nov': '11', 'december': '12', 'dec': '12'
                    }
                    month = month_dict.get(month_name.lower(), '01')
                    pub_date = f"{year}-{month}"
                else:
                    # If still not found, just find year
                    year_match = re.search(r'(20\d{2})|(19\d{2})', text)
                    if year_match:
                        pub_date = year_match.group(0)
        
        return journal, pub_date
    except:
        return None, None

def format_date(date_str):
    """Format date string to a more readable format"""
    if not date_str or date_str == 'Unknown Date':
        return 'Unknown Date'
        
    # Try different date formats
    formats = [
        '%Y-%m-%d',  # 2023-01-15
        '%Y-%m',     # 2023-01
        '%Y %b %d',  # 2023 Jan 15
        '%Y %B %d',  # 2023 January 15
        '%b %Y',     # Jan 2023
        '%B %Y',     # January 2023
        '%Y'         # 2023
    ]
    
    # Special case: if it's only a year, return as is
    if re.match('^[0-9]{4}$', date_str):
        return date_str
    
    # Try to parse the date
    for fmt in formats:
        try:
            date_obj = datetime.strptime(date_str, fmt)
            # Format as Month Day, Year if we have day info
            if '%d' in fmt:
                return date_obj.strftime('%B %d, %Y')
            # Format as Month Year if we have month info
            elif '%b' in fmt or '%B' in fmt or '%m' in fmt:
                return date_obj.strftime('%B %Y')
            # Otherwise just return year
            else:
                return date_obj.strftime('%Y')
        except ValueError:
            continue
    
    # If we couldn't parse it with standard formats, try to extract parts
    parts = re.split(r'[-/ ]', date_str)
    if len(parts) >= 3:
        # Assume YYYY-MM-DD format
        try:
            year, month, day = parts[0], parts[1], parts[2]
            month_int = int(month)
            if 1 <= month_int <= 12:
                month_name = datetime(2000, month_int, 1).strftime('%B')
                return f"{month_name} {int(day)}, {year}"
        except:
            pass
    
    # If all else fails, return the original string
    return date_str

def update_study(study):
    journal_elem = study.find(string='Journal:').find_next('br').previous_sibling.strip()
    date_elem = study.find(string='Publication Date:').find_next('br').previous_sibling.strip()
    
    if 'Unknown Journal' not in journal_elem and 'Unknown Date' not in date_elem:
        return
    
    source_link = study.find('a', string='View Source')
    if not source_link:
        return
    
    url = source_link['href']
    print(f"Processing: {url}")
    
    journal = None
    pub_date = None
    
    # PubMed
    if 'pubmed.ncbi.nlm.nih.gov' in url:
        pmid = url.split('/')[-2] if url.endswith('/') else url.split('/')[-1]
        journal, pub_date = get_pubmed_info(pmid)
    
    # Semantic Scholar
    elif 'semanticscholar.org' in url:
        paper_id = url.split('/')[-1]
        journal, pub_date = get_semanticscholar_info(paper_id)
    
    # Other URLs
    else:
        # Try to extract DOI
        doi = None
        if 'doi.org' in url:
            doi = url.split('doi.org/')[-1]
        else:
            doi = extract_doi_from_webpage(url)
        
        if doi:
            journal, pub_date = get_crossref_info(doi)
    
    # If still not found, try PDF
    if not journal or not pub_date:
        pdf_link = study.find('a', string='Download PDF')
        if pdf_link and 'pdf' in pdf_link['href']:
            journal, pub_date = parse_pdf(pdf_link['href'])
    
    # Update HTML
    if journal and 'Unknown Journal' in journal_elem:
        new_journal = journal_elem.replace('Unknown Journal', journal)
        study.find(string='Journal:').find_next('br').previous_sibling.replace_with(new_journal)
    
    if pub_date and 'Unknown Date' in date_elem:
        # Format the date nicely and add a space before it
        formatted_date = format_date(pub_date)
        new_date = date_elem.replace('Unknown Date', f" {formatted_date}")
        study.find(string='Publication Date:').find_next('br').previous_sibling.replace_with(new_date)
    
    time.sleep(1)  # Be polite with requests

def get_html_files():
    # Get all HTML files in the current directory
    html_files = [f for f in os.listdir('.') if f.endswith('.html')]
    return html_files

def select_html_file(html_files):
    print("Available HTML files:")
    for i, file in enumerate(html_files):
        print(f"{i+1}. {file}")
    
    choice = input("Enter the number of the file to process (or press Enter for the first file): ")
    if choice.strip() == "":
        return html_files[0]
    
    try:
        index = int(choice) - 1
        if 0 <= index < len(html_files):
            return html_files[index]
        else:
            print("Invalid choice. Using the first file.")
            return html_files[0]
    except ValueError:
        print("Invalid input. Using the first file.")
        return html_files[0]

def main():
    parser = argparse.ArgumentParser(description='Update study information in an HTML file.')
    parser.add_argument('--input', '-i', help='Input HTML file path')
    parser.add_argument('--output', '-o', help='Output HTML file path (default: updated_[input file])')
    args = parser.parse_args()
    
    input_file = args.input
    
    # If no input file is specified, look for HTML files in the current directory
    if not input_file:
        html_files = get_html_files()
        if not html_files:
            print("No HTML files found in the current directory.")
            return
        
        input_file = select_html_file(html_files)
    
    # Set output file name
    output_file = args.output
    if not output_file:
        filename = os.path.basename(input_file)
        output_file = f"updated_{filename}"
    
    print(f"Processing file: {input_file}")
    print(f"Output will be saved to: {output_file}")
    
    # Process the HTML file
    with open(input_file, 'r', encoding='utf-8') as f:
        soup = BeautifulSoup(f, 'html.parser')

    studies = soup.find_all('div', class_='study')
    print(f"Found {len(studies)} studies to process.")
    
    for i, study in enumerate(studies):
        print(f"Processing study {i+1}/{len(studies)}")
        update_study(study)
    
    # Save modified HTML
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(str(soup))
    
    print(f"Processing complete. Output saved to {output_file}")

if __name__ == '__main__':
    main()
